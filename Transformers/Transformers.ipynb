{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Fb5GfHG9aYdj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adxWDxNiaYdl"
      },
      "source": [
        "X is an input matrix of dimensions (t,k) where\n",
        "t-> no of input vectors\n",
        "k-> dimension of each vector\n",
        "and we'll have the input vectors and calculate their output vectors in batches\n",
        "so the overall dimensions of the input matrix X would be X(b,t,k) where b is the batch size. (no of t-word sentences in a batch.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3sN3q2raYdn"
      },
      "outputs": [],
      "source": [
        "X = ...\n",
        "\n",
        "raw_weights = torch.bmm(X,X.transpose(1,2))\n",
        "# transpose(1,2) swaps the matrix's second and third dimensions.(the first here is batch size.)\n",
        "# bmm is batch matrix multiplication.\n",
        "weights = F.softmax(raw_weights,dim=2) # means softmax would be calculated for each row, throughout columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lYfx9IfaYdn"
      },
      "outputs": [],
      "source": [
        "Y = torch.bmm(X,weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be8iNF-maYdn"
      },
      "outputs": [],
      "source": [
        "#define the self attention class: the complete layer of multiple heads\n",
        "#whatever was vertically dimensioned in the theory, is now horizontal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6jesnTdEaYdo"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self,k,heads):\n",
        "        super().__init__()\n",
        "        assert k%heads==0\n",
        "        self.k,self.heads=k,heads\n",
        "        #The linear function creates a transformation layer which returns a transformed output for an input. y = x(W.T)\n",
        "        self.toKeys = nn.Linear(k,k,bias=False) #wk\n",
        "        self.toQueries = nn.Linear(k,k,bias=False) #wq\n",
        "        self.toValues = nn.Linear(k,k,bias=False) #wv\n",
        "        #to concatenate the resultant chunks of each attention head\n",
        "        self.unifyHeads = nn.Linear(k,k)\n",
        "    def forward(self,x):\n",
        "        #the input would be a 3-d vector of form (batch_size,seq_len,in_features) because the sequence of input vectors comes in batches.\n",
        "        b,t,k=x.size()\n",
        "        h=self.heads\n",
        "        #but how does the matrix multiplication work on 3d matrix such as x: it also gives out a 3d matrix of form (batch_size,seq_len,out_features)\n",
        "        #here, both in and out features are k\n",
        "        queries=self.toQueries(x)\n",
        "        keys = self.toKeys(x)\n",
        "        values = self.toValues(x)\n",
        "\n",
        "        headSize = self.k//self.heads\n",
        "\n",
        "        #This simply reshapes the tensors to break the last dimension into two dimensions.\n",
        "        #purpose: to divide the features of each input vector into h parts, so that each head receives one chunk of that input vector.\n",
        "        #the chunks are of low dimensions and easier to compute individually.\n",
        "        keys = keys.view(b,t,h,headSize)\n",
        "        queries = queries.view(b,t,h,headSize)\n",
        "        values = values.view(b,t,h,headSize)\n",
        "\n",
        "        # - fold heads into the batch dimension=> needed to compute the dot product parallely\n",
        "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, headSize)\n",
        "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, headSize)\n",
        "        values = values.transpose(1, 2).contiguous().view(b * h, t, headSize)\n",
        "        # now the first dimension is size of the batch: each batch has sequences of mini-vectors(one vector for each head).\n",
        "        # size of sequence is the block size.\n",
        "        # for each sequence, we have corresponding outputs\n",
        "        # how to concatenate those outputs once you have transformed the matrix? we un-transform it first before concatenating.\n",
        "\n",
        "        #compute weights\n",
        "        raw_weights = torch.bmm(queries,keys.transpose(1,2))\n",
        "        #raw_weights is of dimension: b*h, t, t\n",
        "\n",
        "        raw_weights /= headSize**(1/2)\n",
        "        weights = F.softmax(raw_weights,dim=2)\n",
        "\n",
        "        #apply self-attention to the input vectors\n",
        "        out = torch.bmm(weights, values).view(b,h,t,headSize)\n",
        "        out = out.transpose(1,2).view(b,t,h*headSize)\n",
        "        #unifyHeads is not really necessary once we do h*headSize\n",
        "        return self.unifyHeads(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJIyPvIraYdo"
      },
      "source": [
        "![alt text](resizing.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHb3CwyZaYdo"
      },
      "source": [
        "Now, if we combine self attention with some other mechanisms, we can build a transformer block, which we can repeat to achieve better results.\n",
        "Still don't understand how connecting a bunch of components works out?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,k,heads):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention=SelfAttention(k,heads=heads)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(k)\n",
        "    self.norm2 = nn.LayerNorm(k)\n",
        "\n",
        "    self.ff = nn.Sequential(\n",
        "        nn.Linear(k,4*k),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*k,k)\n",
        "    )\n",
        "    #but sequential's units will output a number, not a vector? yep and a vector is k numbers\n",
        "  def forward(self,x):\n",
        "    attended = self.attention(x)\n",
        "    x=self.norm1(attended+x)\n",
        "\n",
        "    fedForward = self.ff(x)\n",
        "    return self.norm2(fedForward+x)\n"
      ],
      "metadata": {
        "id": "ULbNifTzbI3m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CTransformer(nn.Module):\n",
        "  def __init__(self,k,heads,depth,seq_length,num_tokens,num_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    #layer for handling input\n",
        "    self.num_tokens #size of vocabulary i.e. no of unique tokens that the transformer knows.\n",
        "    self.token_emb = nn.Embedding(num_tokens,k) # map each token(integer) to a size k vector.\n",
        "    self.pos_emb = nn.Embedding(seq_length,k) # map each position (0->seq_length-1) to a size k vector\n",
        "    #these embedding layers will be initialized randomly, but trained with the input.\n",
        "\n",
        "    #transformer blocks\n",
        "    tblocks=[]\n",
        "    for i in range(depth):\n",
        "      tblocks.append(TransformerBlock(k,heads))\n",
        "    self.tblocks=nn.Sequential(*tblocks)\n",
        "\n",
        "    #layer for handling output: project to an array of size num_classes\n",
        "    self.toProbs = nn.Linear(k,num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # process the input before feeding to transformer blocks\n",
        "    \"\"\"\n",
        "        :param x: A (b, t) tensor of integer values representing\n",
        "                  words (in some predetermined vocabulary).\n",
        "                  Each batch has t tokens. so each batch corresponds to 1 sequence?\n",
        "        :return: A (b, c) tensor of log-probabilities over the\n",
        "                 classes (where c is the nr. of classes).\n",
        "                 Probability distribution over c classes for each batch\n",
        "    \"\"\"\n",
        "    tokens = self.token_emb(x)\n",
        "    b, t, k = tokens.size()\n",
        "    positions = torch.arange(t)\n",
        "    positions = self.pos_emb(positions)[None,:,:].expand(b,t,k)\n",
        "    '''\n",
        "    [None,:,:]\n",
        "    This adds a new dimension at the beginning of the tensor.\n",
        "    It changes the shape from (t, k) to (1, t, k).\n",
        "    expand(b,t,k)\n",
        "    This expands the tensor to shape (b, t, k), where b is the batch size.\n",
        "    It repeats the positional embeddings b times along the first dimension.\n",
        "    This operation doesn't allocate new memory; it creates a view of the original tensor.\n",
        "    '''\n",
        "    x = tokens+positions\n",
        "\n",
        "    x = self.tblocks(x)\n",
        "\n",
        "    #process the output before returning\n",
        "    x = x.mean(dim=1) # calculates mean over the second dimension ie t. Now x is of shape (b,k) because each batch only has one vector\n",
        "    x = self.toProbs(x) # projects x to a shape (b,1,num_classes)\n",
        "    return F.log_softmax(x,dim=1) # calculates log of softmax across the second dimension ie num_classes. log is easier to handle than actual probability calculation\n",
        "    #done according to the task of the transformer."
      ],
      "metadata": {
        "id": "X5sdL_lVcWLU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "txpvSbmUTXqa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}